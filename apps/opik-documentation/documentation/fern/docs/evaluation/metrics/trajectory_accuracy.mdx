---
description: Score whether an agent followed the expected action path
---

# Trajectory Accuracy

`TrajectoryAccuracy` checks how closely an agent’s sequence of states or tool calls matches an expected path. It is useful for workflow agents where order matters—for example, verifying that a customer-support agent searched the knowledge base before responding.

```python title="Auditing an agent run"
from opik.evaluation.metrics import TrajectoryAccuracy

expected = ["gather_requirements", "search_docs", "draft_response", "send_summary"]
actual = ["gather_requirements", "draft_response", "send_summary"]

metric = TrajectoryAccuracy(expected_path=expected)
score = metric.score(output=actual)

print(score.value)
print(score.reason)
```

## Inputs

| Argument | Type | Required | Description |
| --- | --- | --- | --- |
| `output` | `Sequence[str]` | **Yes** | Observed trajectory (e.g., list of tool or state names). |
| `expected_path` | `Sequence[str]` | **Yes** | Expected golden-path sequence supplied when instantiating the metric. |

## Configuration

| Parameter | Default | Notes |
| --- | --- | --- |
| `expected_path` | — | Required. Defines the canonical sequence. |
| `allow_skips` | `False` | When `True`, ignores steps missing at the end of the run. |
| `name` | `trajectory_accuracy` | Override if you need a custom metric name. |

The score is normalised between 0.0 and 1.0 and the `reason` explains which steps were missing or out of order.
