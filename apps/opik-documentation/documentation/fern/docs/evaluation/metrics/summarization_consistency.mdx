---
description: Ensure autogenerated summaries stay faithful to the source content
---

# Summarization Consistency Judge

`SummarizationConsistencyJudge` compares a generated summary with the original document (or transcript) and scores how faithfully key facts were preserved. It follows the GEval method: expanding your instructions into a chain-of-thought rubric, then grading on a 0.0–1.0 scale with detailed explanations.

Use it when you automatically summarise support tickets, research reports, or call transcripts and want to catch hallucinations before they reach end users.

```python title="Checking summary faithfulness"
from opik.evaluation.metrics import SummarizationConsistencyJudge

metric = SummarizationConsistencyJudge(model="gpt-4o")

score = metric.score(
    input="""CONTEXT:\nAcme's Q2 revenue grew 12% thanks to the launch of Product Vega.\nOperating margin declined to 14% because of R&D hiring.\n""",
    output="""Acme's revenue was flat but margins improved due to new hires.""",
)

print(score.value)
print(score.reason)
```

## Inputs

| Argument | Type | Required | Description |
| --- | --- | --- | --- |
| `input` | `str` | Optional | Source document or context. |
| `output` | `str` | **Yes** | Summary to evaluate. |
| `context` | `str | list[str]` | Optional | Additional evidence the summary should stay aligned with. |

## Configuration

| Parameter | Default | Notes |
| --- | --- | --- |
| `model` | `gpt-5-nano` | Swap to a larger evaluator for longer or more technical content. |
| `temperature` | `0.0` | Keep low for deterministic scoring; raise slightly to sample different critiques. |
| `track` | `True` | Disable to skip sending traces to Opik. |
| `project_name` | `None` | Override when logging scores. |

The returned metadata includes the evaluator’s rubric notes, making it easy to debug why a summary received a low score.
